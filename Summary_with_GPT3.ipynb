{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244466b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22903d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"<OPENAI_API_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a28f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efb2d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open('GPT3.pdf')   # creates Document object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe8c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve full text from pdf file\n",
    "\n",
    "full_text = ''\n",
    "for page in doc:\n",
    "    text = page.get_text(\"text\")\n",
    "    full_text += text + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd45641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction\n",
      "Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly\n",
      "ﬂexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word\n",
      "vectors [MCCD13, PSM14] and fed to task-speciﬁc architectures, then RNNs with multiple layers of representations\n",
      "and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to\n",
      "task-speciﬁc architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have\n",
      "been directly ﬁne-tuned, entirely removing the need for task-speciﬁc architectures [RNSS18, DCLT18, HR18].\n",
      "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension,\n",
      "question answering, textual entailment, and many others, and has continued to advance based on new architectures\n",
      "and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while\n",
      "the architecture is task-agnostic, there is still a need for task-speciﬁc datasets and task-speciﬁc ﬁne-tuning: to achieve\n",
      "strong performance on a desired task typically requires ﬁne-tuning on a dataset of thousands to hundreds of thousands\n",
      "of examples speciﬁc to that task. Removing this limitation would be desirable, for several reasons.\n",
      "First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the\n",
      "applicability of language models. There exists a very wide range of possible useful language tasks, encompassing\n",
      "anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many\n",
      "of these tasks it is difﬁcult to collect a large supervised training dataset, especially when the process must be repeated\n",
      "for every new task.\n",
      "Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness\n",
      "of the model and the narrowness of the training distribution. This can create problems for the pre-training plus\n",
      "ﬁne-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then\n",
      "ﬁne-tuned on very narrow task distributions. For instance [HLW+20] observe that larger models do not necessarily\n",
      "generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm\n",
      "can be poor because the model is overly speciﬁc to the training distribution and does not generalize well outside it\n",
      "[YdC+19, MPL19]. Thus, the performance of ﬁne-tuned models on speciﬁc benchmarks, even when it is nominally at\n",
      "human-level, may exaggerate actual performance on the underlying task [GSL+18, NK19].\n",
      "Third, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural\n",
      "language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number\n",
      "of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often\n",
      "Figure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad\n",
      "set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize\n",
      "the desired task. We use the term “in-context learning” to describe the inner loop of this process, which occurs within\n",
      "the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a\n",
      "model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded\n",
      "within a single sequence.\n",
      "3\n",
      " Figure 1.2: Larger models make increasingly efﬁcient use of in-context information. We show in-context learning\n",
      "performance on a simple task requiring the model to remove random symbols from a word, both with and without a\n",
      "natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate\n",
      "improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\n",
      "of tasks.\n",
      "sufﬁcient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing\n",
      "to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans\n",
      "to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy\n",
      "dialogue. To be broadly useful, we would someday like our NLP systems to have this same ﬂuidity and generality.\n",
      "One potential route towards addressing these issues is meta-learning1 – which in the context of language models means\n",
      "the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities\n",
      "at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [RWC+19]\n",
      "attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form\n",
      "of task speciﬁcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task\n",
      "and is then expected to complete further instances of the task simply by predicting what comes next.\n",
      "While it has shown some initial promise, this approach still achieves results far inferior to ﬁne-tuning – for example\n",
      "[RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind\n",
      "the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of\n",
      "solving language tasks.\n",
      "Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer\n",
      "language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters\n",
      "[DCLT18], to 1.5 billion parameters [RWC+19], to 8 billion parameters [SPP+19], 11 billion parameters [RSR+19],\n",
      "and ﬁnally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream\n",
      "NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a\n",
      "smooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and\n",
      "tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong\n",
      "gains with scale.\n",
      "1In the context of language models this has sometimes been called “zero-shot transfer”, but this term is potentially ambiguous:\n",
      "the method is “zero-shot” in the sense that no gradient updates are performed, but it often involves providing inference-time\n",
      "demonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term “meta-learning”\n",
      "to capture the inner-loop / outer-loop structure of the general method, and the term “in context-learning” to refer to the inner\n",
      "loop of meta-learning. We further specialize the description to “zero-shot”, “one-shot”, or “few-shot” depending on how many\n",
      "demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model\n",
      "learns new tasks from scratch at inference time or simply recognizes patterns seen during training – this is an important issue which\n",
      "we discuss later in the paper, but “meta-learning” is intended to encompass both possibilities, and simply describes the inner-outer\n",
      "loop structure.\n",
      "4\n",
      " Figure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance\n",
      "improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are\n",
      "more proﬁcient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP\n",
      "benchmark suite.\n",
      "In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call\n",
      "GPT-3, and measuring its in-context learning abilities. Speciﬁcally, we evaluate GPT-3 on over two dozen NLP datasets,\n",
      "as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training\n",
      "set. For each task, we evaluate GPT-3 under 3 conditions: (a) “few-shot learning”, or in-context learning where we\n",
      "allow as many demonstrations as will ﬁt into the model’s context window (typically 10 to 100), (b) “one-shot learning”,\n",
      "where we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only\n",
      "an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional\n",
      "ﬁne-tuning setting, but we leave this to future work.\n",
      "Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to\n",
      "remove extraneous symbols from a word. Model performance improves with the addition of a natural language task\n",
      "description, and with the number of examples in the model’s context, K. Few-shot learning also improves dramatically\n",
      "with model size. Though the results in this case are particularly striking, the general trends with both model size and\n",
      "number of examples in-context hold for most tasks we study. We emphasize that these “learning” curves involve no\n",
      "gradient updates or ﬁne-tuning, just increasing numbers of demonstrations given as conditioning.\n",
      "Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot\n",
      "setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held\n",
      "by ﬁne-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in\n",
      "the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the\n",
      "zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art\n",
      "relative to ﬁne-tuned models operating in the same closed-book setting.\n",
      "GPT-3 also displays one-shot and few-shot proﬁciency at tasks designed to test rapid adaption or on-the-ﬂy reasoning,\n",
      "which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them\n",
      "deﬁned only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human\n",
      "evaluators have difﬁculty distinguishing from human-generated articles.\n",
      "At the same time, we also ﬁnd some tasks on which few-shot performance struggles, even at the scale of GPT-3. This\n",
      "includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE\n",
      "or QuAC. By presenting a broad characterization of GPT-3’s strengths and weaknesses, including these limitations, we\n",
      "hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.\n",
      "A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should\n",
      "not be seen as a rigorous or meaningful benchmark in itself).\n",
      "5\n",
      " We also undertake a systematic study of “data contamination” – a growing problem when training high capacity models\n",
      "on datasets such as Common Crawl, which can potentially include content from test datasets simply because such\n",
      "content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify\n",
      "its distorting effects. Although we ﬁnd that data contamination has a minimal effect on GPT-3’s performance on most\n",
      "datasets, we do identify a few datasets where it could be inﬂating results, and we either do not report results on these\n",
      "datasets or we note them with an asterisk, depending on the severity.\n",
      "In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion\n"
     ]
    }
   ],
   "source": [
    "print(full_text[4771:16811])     # text starting at 'Introduction' until maximum length accepted by GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2325f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = full_text[4771:16811] + \"\\n Tl;dr:\"   # pass maximum length of text starting at 'Content' to gpt3 summarizer\n",
    "response = openai.Completion.create(\n",
    "    model = \"text-davinci-003\",\n",
    "    prompt = prompt,\n",
    "    temperature = 0.7,\n",
    "    max_tokens = 800,  # length of summary\n",
    "    top_p = 0.9,\n",
    "    frequency_penalty = 0.0,\n",
    "   presence_penalty = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e02299d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We train a 175 billion parameter language model, GPT-3, and measure its in-context learning abilities on over two dozen NLP datasets. We find that GPT-3 is sometimes competitive with or even surpasses state-of-the-art results on these tasks when given few demonstrations. We also find that it is capable of one-shot and few-shot proﬁciency at tasks designed to test rapid adaption or on-the-ﬂy reasoning. We present a systematic study of data contamination and discuss the potential implications of our results.\n",
      " parameters) and evaluate them on the same suite of tasks, allowing us to track performance gains with respect to model\n",
      "size. This analysis reveals that larger models make increasingly efﬁcient use of contextual information.\n",
      "We hope this paper will help inform future studies of meta-learning and few-shot learning in language models, and\n",
      "draw attention to the potential of large scale models for both text synthesis and downstream tasks. Our code, datasets,\n",
      "and pre-trained models are available here2.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43b400cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\n",
      "on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic\n",
      "in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of\n",
      "thousands of examples. By contrast, humans can generally perform a new language task from only\n",
      "a few examples or from simple instructions – something which current NLP systems still largely\n",
      "struggle to do. Here we show that scaling up language models greatly improves task-agnostic,\n",
      "few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁne-\n",
      "tuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion\n",
      "parameters, 10x more than any previous non-sparse language model, and test its performance in\n",
      "the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning,\n",
      "with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3\n",
      "achieves strong performance on many NLP datasets, including translation, question-answering, and\n",
      "cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as\n",
      "unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\n",
      "time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\n",
      "datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\n",
      "we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty\n",
      "distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding\n",
      "and of GPT-3 in general.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "original_abstract = full_text[507:2286]\n",
    "print(original_abstract)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
