{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244466b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22903d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"<OPENAI_API_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a28f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efb2d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = fitz.open('GPT3.pdf')   # creates Document object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe8c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process with chunks of 1 page at a time\n",
    "\n",
    "summary_list = []\n",
    "\n",
    "for page in doc:\n",
    "    text = page.get_text(\"text\")\n",
    "    if len(text) > 6000:    # limit prompt size to fit model\n",
    "        text = text[:6000]\n",
    "    prompt = text + '\\n Tl;dr:'\n",
    "    response = openai.Completion.create(\n",
    "        model = \"text-davinci-003\",\n",
    "        prompt = prompt,\n",
    "        temperature = 0.7,\n",
    "        max_tokens = 120,  # length of summary\n",
    "        top_p = 0.9,\n",
    "        frequency_penalty = 0.0,\n",
    "        presence_penalty = 1\n",
    "    )\n",
    "    summary_list.append(response[\"choices\"][0][\"text\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2325f4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We show that large language models are capable of few-shot learning, achieving strong performance on many NLP tasks without any gradient updates or fine-tuning. We demonstrate this using GPT-3, an autoregressive language model with 175 billion parameters, which can perform a wide range of tasks from just a few examples or simple instructions. We also identify datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues due to training on large web corpora. Finally, we find that GPT-3 can generate  We introduce GPT-3, a new language model that uses deep learning to produce human-like text. We evaluate GPT-3 on a variety of tasks ranging from language modeling and question answering to translation and common sense reasoning. Our results show that GPT-3 can outperform existing models on many of these tasks, and in some cases can even approach human performance. We also discuss potential risks and limitations of GPT-3 and propose methods for measuring and preventing memorization of benchmark datasets. \n",
      " In recent years, pre-trained language representations have been used in increasingly flexible and task-agnostic ways for downstream transfer. This has led to progress on many challenging NLP tasks. However, there is still a need for task-specific datasets and task-specific fine-tuning for each new task, which can be limiting and lead to the exploitation of spurious correlations. To address this, researchers are looking into language model meta-learning, which allows models to rapidly adapt to new tasks using abilities developed during unsupervised pre-training.  In-context learning is a form of meta-learning which uses the text input of a pretrained language model as a form of task specification. Recent work has shown some initial promise, but it still achieves results far inferior to fine-tuning. Increasing the capacity of transformer language models may offer a way forward, as there is evidence suggesting that log loss follows a smooth trend of improvement with scale.  In this paper, we tested the hypothesis that larger language models are better at in-context learning by training a 175 billion parameter autoregressive language model (GPT-3). We evaluated GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set. Results showed that GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the few-shot setting is sometimes competitive with or even surpasses state-of-the-art. It also displays  We present a systematic study of data contamination and its effect on GPT-3's performance. We develop tools to measure data contamination and quantify its distorting effects. Although we find that data contamination has a minimal effect on GPT-3's performance on most datasets, we do identify a few datasets where it could be inflating results. We also train a series of smaller models ranging from 125 million parameters to 13 billion parameters in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Finally, we discuss concerns about bias, fairness, and broader societal  This figure contrasts traditional fine-tuning with zero-shot, one-shot, and few-shot learning methods. Zero-shot requires the model to perform a task without any prior examples while one-shot allows for demonstration of the task. Few-shot requires only a few dozen examples and is compared to human performance in some cases. Sections 2.1-2.4 discuss the details of the models, training data, training process, and evaluation respectively.  We trained 8 different models with varying sizes, ranging from 125 million parameters to 175 billion parameters. We used the same model and architecture as GPT-2, and trained our models on a mixture of datasets including CommonCrawl, Books1, Books2 and English Wikipedia. The training data was filtered and deduplicated to improve average quality.  We trained GPT-3 on a variety of datasets and used model parallelism to reduce memory requirements. We also carefully monitored gradient noise scale to ensure appropriate batch size. We removed data contamination from downstream tasks, but due to the cost of training, some overlaps may remain.  We evaluate the 8 models described in Section 2 on a wide range of datasets, grouped into 9 categories representing roughly similar tasks. In Sections 3.1-3.9 we evaluate traditional language modeling tasks, closed book question answering tasks, translation tasks, Winograd Schema-like tasks, commonsense reasoning or question answering tasks, reading comprehension tasks, the SuperGLUE benchmark suite, NLI, and additional tasks designed especially to probe in-context learning abilities. We evaluate all tasks in the few-shot, one-shot, and zero-shot settings.  GPT-3 achieves a new state of the art on the Penn Tree Bank (PTB) language modeling dataset with a perplexity of 20.50, and sets a new record on the LAMBADA dataset by a substantial margin of 1.5%. The performance scaling of GPT-3 follows a power-law trend with increasing compute used for training. \n",
      "GPT-3 significantly improves SOTA on LAMBADA, achieving a strong boost to accuracy in the few-shot setting. Zero-shot performance is 8% better than the previous state of the art, and few-shot performance increases 18%. Model size is important for few-shot performance, with the smallest model seeing a 20% decrease in accuracy, while GPT-3 improves by 10%. The fill-in-the-blank method does not work well in one-shot, likely because it requires several examples to recognize the pattern.  GPT-3 achieved good results on three open-domain QA tasks. In the closed-book setting, GPT-3 achieved 64.3% in the zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting on TriviaQA. On WebQuestions, GPT-3 achieved 14.4% in the zero-shot setting, 25.3% in the one-shot setting, and 41.5% in the few-shot setting. Compared to TriviaQA, WebQS  GPT-3's performance on translation tasks grows smoothly with model size, surpassing unsupervised NMT results in the one and few-shot settings. It shows evidence of multilingual capability despite its primarily English training data, suggesting that it is able to learn from a natural blend of languages.  Few-shot GPT-3 outperforms previous unsupervised NMT work by up to 5 BLEU when translating into English, reflecting its strength as an English language model. Results are reported for the WMT'14 Fr↔En, WMT'16 De↔En, and WMT'16 Ro↔En datasets as measured by multi-bleu.perl with XLM's tokenization in order to compare most closely with prior unsupervised NMT work. SacreBLEU results are reported in Appendix H. Underline indicates an unsupervised or  GPT-3 achieved near human performance on the Winograd Schema Challenge, with gains to few-shot learning increasing with model size. GPT-3 also showed improved performance across translation tasks with zero-, one-, and few-shot settings, showing a smooth trend of improvement with model capacity.  GPT-3 achieves strong results on common sense reasoning tasks such as PIQA, ARC (Easy and Challenge), and OpenBookQA. On Winograd, GPT-3 achieves 88.3%, 89.7%, and 88.6% in the zero-shot, one-shot, and few-shot settings, respectively, with no clear gains from in-context learning. On the more difficult Winogrande dataset, GPT-3 achieves 70.2% in the zero-shot setting, 73.2% in the one-shot setting, and 77  GPT-3 shows mixed results on common sense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC. However, it significantly improved OpenBookQA and sets a new SOTA on PIQA. On reading comprehension tasks, GPT-3 performs best on CoQA and worst on QuAC. It outperforms a fine-tuned BERT baseline on DROP and slightly outperforms the best fine-tuned result on SQuAD 2.0. On SuperGLUE, GPT-3  GPT-3 achieves 85 F1 on the CoQA reading comprehension task in the few-shot setting, only a few points behind measured human performance and state-of-the-art fine-tuned models. On SuperGLUE, GPT-3 few-shot performs competitively with the fine-tuned baselines and SOTA, achieving 75.4 F1 on MultiRC and 90.2 accuracy on ReCoRD.  GPT-3 achieves near-SOTA performance on COPA and ReCoRD in the one-shot and few-shot settings. On WSC, GPT-3 performs well with 80.1% accuracy in the few-shot setting. On BoolQ, MultiRC, and RTE, performance is reasonable, roughly matching that of a fine-tuned BERT-Large. GPT-3 performance increases with model size and number of examples in context, and requires less than eight total examples per task to outperform a fine-tuned BERT-Large on overall  GPT-3 performs poorly on tasks such as RTE and ANLI, suggesting that NLI is still a difficult task for language models. We also tested GPT-3 on several synthetic and qualitative tasks, including arithmetic operations, letter rearrangements, SAT-style analogy problems, and English grammar corrections. Results indicate that GPT-3 is able to solve these tasks in the few-shot setting, though with varying degrees of success.  GPT-3 displays strong proficiency when the number of digits is small, achieving 100% accuracy on 2 digit addition and subtraction and 80.2% accuracy on 3 digit addition and 94.2% accuracy on 3-digit subtraction. GPT-3 also achieves 29.2% accuracy at 2 digit multiplication and 21.3% accuracy at single digit combined operations. Smaller models do not perform as well, but the full GPT-3 model still achieves a significant amount of accuracy even in one-shot and zero-shot settings.  GPT-3 is able to perform basic arithmetic tasks with accuracy in all three settings: zero-shot, one-shot, and few-shot. It also displays good performance on word manipulation tasks such as unscrambling and reversing words. The full GPT-3 model achieved the highest scores in all tasks. \n",
      "GPT-3 achieved 65.2% in the few-shot setting, 59.1% in the one-shot setting, and 53.7% in the zero-shot setting on a set of 374 SAT analogies. The results improved with scale, with the full 175 billion model improving by over 10% compared to the 13 billion parameter model.  We tested human ability to distinguish GPT-3 generated news articles from real ones by asking participants to select whether the article was \"very likely written by a human\", \"more likely written by a human\", \"I don't know\", \"more likely written by a machine\" or \"very likely written by a machine\". We found that humans could detect GPT-3 generated articles with high accuracy, with 75% of participants correctly identifying them.  Human accuracy at detecting model-generated text decreases as the model size increases, ranging from 86% for a deliberately bad control model to 52% for GPT-3 175B. A two-sample T-Test was used to compare the mean accuracy between each model and the control model. Examples of synthetic articles from GPT-3 were provided, along with indicators such as factual inaccuracies and repetition that could be used to detect model-generated text. Automatic discriminators may be more successful at detecting model-generated text than human evaluators.  People's ability to identify model-generated news articles decreased significantly when using GPT-3 175B compared to a control model, with mean accuracy of 88% for the control model and 52% for GPT-3 175B. A two-sample T-Test showed a p-value of 12.7 (3.2e-23) for the difference in mean accuracy between GPT-3 175B and the control model. There was also a 10.6% rate of \"I don't know\" assignments.  The United Methodist Church has agreed to split, with those who oppose gay marriage forming their own denomination. The split is the second in the church's history and comes at a time when it has been losing members. Megyn Kelly criticized Joaquin Phoenix for his promise to wear a tuxedo to awards events, but he said he was committed to it.  GPT-3 appears to be proficient at the task of correcting English grammar in a few-shot setting, correctly identifying and correcting errors in the five sentences presented. The model was able to correct errors ranging from simple spelling and punctuation mistakes to more complex structural problems, such as incorrect verb tense and incorrect word order. \n",
      "GPT-3 can be used to correct English grammar with few-shot examples. It can accurately complete sentences that have poor English input and provide the correct good English output. It may make assumptions about what is \"good\" English that can lead to errors, such as removing the word \"cheap\" from a sentence about renting a house.  We measure model performance during training on a deduplicated validation split of our training distribution, and find that the gap between training and validation performance grows only minimally with model size and training time. To investigate potential contamination, we produce clean versions of all benchmarks studied in this paper, and compare to original scores. We find that contamination, if present, does not have a significant effect on reported results, though there are a few cases where further investigation is needed.  We constructed cleaned versions of each benchmark to check for potential contamination in our training set. We found evidence that the PIQA and Winograd results were contaminated, so we marked these results with an asterisk in Section 3. We found no evidence that other benchmarks were affected.  We found that the 4 Wikipedia language modeling benchmarks and the Children's Book Test dataset are almost entirely contained in our training data. We do not report results on these datasets due to contamination. We also inspected datasets where contamination was high, but with minimal impact on performance, suggesting false positives. We note that Penn Tree Bank was unaffected and became our chief language modeling benchmark. We have made a best effort to measure and document the effects of data contamination. GPT-3 has notable weaknesses in text synthesis and several NLP tasks, and has structural and algorithmic limitations that may explain some of its issues.  GPT-3 has many potential applications, but it also has the potential for deliberate misuse and bias. These issues need to be studied and mitigated. Additionally, energy efficiency should be considered.  Language models have the potential to be used for malicious purposes such as misinformation, spam, phishing, and fraud. Low and mid-skill actors are currently not using language models for these activities, but if the quality of text synthesis improves, then they may become more interested. Advanced persistent threats (APTs) may also be interested in using language models, though this is difficult to predict since they do not typically discuss operations in the open. Incentives like scalability and ease of use could make language models attractive to malicious actors. Researchers can work on mitigation research, prototyping, and coordinating with  An analysis of biases in GPT-3 shows that occupations have a higher probability of being associated with male identifiers than female ones. We also found that when prompted with \"The competent {occupation} was a,\" the majority of occupations had an even higher probability of being followed by a male identifier than a female one. Additionally, when performing pronoun resolution on Winogender dataset, GPT-3 175B had the highest accuracy of all the models and it was the only model where the accuracy for Occupant sentences (sentences where the correct answer was the Occupation option) for females was higher  We used prompts such as \"He was very\", \"She was very\", \"He would be described as\", and \"She would be described as\" to investigate gender bias in GPT-3. We found that words describing males often spanned a greater spectrum than those describing females, which were more likely to be appearance-oriented. We also looked at racial bias, finding that 'Asian' had a consistently high sentiment, while 'Black' had a consistently low sentiment across the models we analyzed. \n",
      "We found that the models make associations with religious terms that indicate a propensity to reflect how these terms are presented in the world. For example, with Islam, words such as ramadan, prophet, and mosque co-occurred at a higher rate than for other religions. Additionally, words such as violent, terrorism, and terrorist co-occurred more often with Islam than with other religions and were in the top 40 most favored words for Islam in GPT-3.  This paper discussed the challenges of measuring biases in large-scale generative models, energy usage associated with training such models, and related work on increasing model size to improve performance. It also highlighted the need for further research into bias mitigation and suggested that a holistic approach should be taken to address this issue. Additionally, it noted that large models can be surprisingly efficient once trained and that there are techniques such as model distillation which can reduce the cost of using such models.  We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, with results in some cases almost matching the performance of fine-tuned models. We also discussed potential approaches to further improve the model, such as task-specific distillation architectures, metalearning, question answering, reading comprehension, and adversarially constructed datasets. Additionally, we discussed techniques such as matching networks, RL2, learning to optimize, MAML, auto-regressive density estimation, semi-supervised  We trained a very large language model, and achieved state-of-the-art results without fine-tuning. We also documented predictable trends of scaling in performance. Despite some limitations and weaknesses, our results suggest that large language models may be an important ingredient in the development of general language systems. We thank Ryan Lowe and other contributors for their feedback, and the OpenAI infrastructure team for making it possible to train models at this scale. \n",
      " Tom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, Jeffrey Wu, Melanie Subbiah, Jared Kaplan, Sam McCandlish, Tom Henighan, Girish Sastry, Alec Radford, Rewon Child, Mark Chen, Aditya Ramesh, Arvind Neelakantan, Pranav Shyam, Sandhini Agarwal, Amanda Askell, Ariel Herbert-Voss, Gretchen Krueger, Benjamin Chess, Clemens Winter, Eric Sig  We used a logistic regression classifier to filter out low-quality documents from the Common Crawl dataset, and fuzzy deduplication to further improve quality. We trained models using Adam with various parameters, and performed test set contamination studies by searching for 13-gram overlaps between test/development sets and our training data.  We removed entire documents that had a single collision, but this overly penalized long documents. We then used Apache Spark to compute exact collisions between test and training sets, and defined \"dirty\" examples as those with any N-gram overlap with any training document. We set a minimum value of 8 words for non-synthetic tasks and a maximum value of 13 for all tasks. We found that GPT-3 is relatively insensitive to contamination.  We present a table that shows statistics on the overlap of datasets with our training corpus, sorted from dirtiest to cleanest. The table includes the metric used, number of examples, the performance of the total, dirty, and clean examples, the count of those examples, the percentage of clean examples, and the relative difference between the performance of the clean vs all examples.  This appendix contains the details of an experiment to measure human ability to distinguish GPT-3-generated synthetic news articles from real news articles. 718 unique participants were recruited through Positly, and 25 news articles were used as inputs for the language models. Outputs were generated for each model with a word count closest to that of the human written article. Results showed that humans are able to accurately distinguish GPT-3-generated articles from real ones.  We conducted experiments to evaluate the human detection of machine generated news articles and found that as model size increases, accuracy decreases and average time spent increases. We used a two-sample t-test for independent groups to compare the means on different runs, and fitted a power law regression line to the graph of average participant accuracy vs model size with 95% confidence intervals estimated from the t-distribution of the sample mean.  We recruited 160 unique US-based participants to take part in two experiments using ∼500 word news articles generated by GPT-3 175B and a control model. We excluded participants due to internet check fails, and found that larger models generate harder-to-distinguish news articles. Additional samples of GPT-3 were generated for a poem writing task, with no additional editing or selection.  Four uncurated poems were generated from a context suggesting the model compose a poem in the style of Wallace Stevens with the title \"Shadows on the Way\". \n",
      "Task phrasing and specifications for the paper involve formatting data from ground truth datasets. The example provided is an informal conversation about different cultures, and tasks ask what should not be done when talking to colleagues from another country, which topics are typically friendly in most places, why people from Asia are more private in conversations, and what the author considers taboo. The correct answer to the last question is \"taboo\". \n",
      "We presented a dataset, ANLI-R2, for training and evaluating natural language inference models. The dataset consists of contexts and questions about the contexts, with three possible answers for each question, one of which is correct. We also provided an example of how to format the dataset for use in training and evaluation of NLI models. \n",
      "In this section, we discussed the different types of datasets that can be used for natural language understanding tasks such as PIQA, COPA, ReCoRD, and ANLI R1. We also provided examples of each dataset type to illustrate how they can be formatted.  We present a formatted dataset for OpenBookQA, HellaSwag, ANLI R3, ARC (Challenge), SAT Analogies and Winograd. The data is formatted in a way that allows easy use by machine learning models. \n",
      "Correct Answer →\n",
      "- [True] An infection in the bloodstream \n",
      "This paper has presented an overview of the various tasks and datasets that have been used to evaluate text generation models. We discussed their purpose, structure, and how they can be used to measure the performance of different models. We also provided examples of formatted datasets for each task to help guide readers in understanding the format of the data. \n",
      "\n",
      "Saint Jean de Brébeuf stayed in New France for 4 years before he went back to France for a few years. \n",
      "Context →\n",
      "The International Organization for Standardization (ISO) is an\n",
      "international standard-setting body composed of representatives from\n",
      "various national standards organizations.\n",
      "Target Completion →\n",
      "International Standard-Setting Organization  In March 1941, the Luftwaffe flew 4,000 sorties and only flew inland missions on moonlit nights. \n",
      "Context →\n",
      "Q: Who is the author of the book “The Wind in the Willows”?\n",
      "A:\n",
      "Target Completion →\n",
      "Kenneth Grahame\n",
      "Target Completion →\n",
      "Kenneth Grahame  Formatted dataset examples for various tasks follow a similar format, with the Context followed by an equals sign and the Target Completion. Examples include WebQA, De→En, En→De, En→Fr, and Fr→En. \n",
      "This figure provides example formatted datasets for a variety of tasks, including translation (Ro→En), arithmetic 1DC, 2D-/2D+/2Dx, 3D-/3D+, and 4D-. Each example shows the context, followed by the target completion.  This paper presents a dataset of formatted arithmetic questions and answers for use in natural language processing applications. The dataset consists of five difficulty levels ranging from 1D+ (single digit addition) to 5D− (five digit subtraction). Each difficulty level contains at least 1000 questions and answers, totaling 5000 in the dataset. The dataset can be used to train AI models to understand and answer simple arithmetic questions.  Results on all tasks for all model sizes show that the 175B model achieved the highest performance, with SOTA scores of 85.6% accuracy on HellaSwag, 68.0% accuracy on LAMBADA, 91.8% accuracy on StoryCloze, 44.5% accuracy on NQs, 68.0% BLEU-mb on Ro→En translation, 45.5% accuracy on WebQs, 39.9% BLEU-mb on Fr→En translation, 45.9% BLEU-sb on En→Fr  Results for all SuperGLUE tasks, SAT task and Winograd tasks are presented in Figures H.1, H.2 and H.3 respectively.  The results of the Arithmetic and Cloze & Completion tasks are shown in Figures H.4 and H.5 respectively. \n",
      "Figure H.1: All results for all NLP tasks.\n",
      "Figure H.2: All results for all Text Generation tasks.\n",
      "Figure H.3: All results for all Language Modeling tasks.\n",
      "Figure H.4: All results for all Dialogue tasks.\n",
      "Figure H.5: All results for all Natural Language Understanding tasks.\n",
      "Figure H.6: All results for all Common Sense Reasoning tasks.\n",
      "Figure H.7: All results for all QA tasks.\n",
      "Figure H.8: All results for all Reading Comprehension tasks.\n",
      "Figure H  The figures show the results of all Scramble and Translation tasks. The results are displayed in a table with the task name, average time, median time, and accuracy rate for each task. \n",
      "This paper provides an overview of recent developments in natural language processing (NLP). It reviews topics such as neural machine translation, bias in NLP, semantic parsing, sentiment analysis, recognizing textual entailment, and multi-task learning. Additionally, it discusses newer approaches such as BERT, Uniter, QuAC, and ARC. This paper provides a good summary of some of the most important advancements in NLP.  Ido Dagan, Oren Glickman, and Bernardo Magnini's 2006 paper \"The PASCAL Recognising Textual Entailment Challenge\" discussed Machine Learning Challenges such as predictive uncertainty, visual object classification, and recognising textual entailment. Mostafa Dehghani et al. (2018) introduced Universal Transformers, and Andrew M. Dai and Quoc V. Le (2015) discussed semi-supervised sequence learning. Nadir Durrani et al. (2014) presented Edinburgh's phrase-based machine translation systems for WMT-14, and Marie-  This paper describes techniques for scaling deep learning models, including data augmentation, knowledge distillation, and model compression. It also provides an empirical analysis of the scalability of deep learning models, demonstrating that the scaling is predictable. Additionally, it explores applications of these techniques to various natural language processing tasks, such as question answering and sentiment analysis. \n",
      "[LCG+19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\n",
      "[LCH+20] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. Adversarial training for \n",
      "This paper provides a summary of recent research related to natural language processing. It includes references to works such as Learned in Translation: Contextualized Word Vectors, Efficient Estimation of Word Representations in Vector Space, A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories, Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering, An Empirical Model of Large-Batch Training, The Penn Treebank: Annotating Predicate Argument Structure, The Natural Language Decathlon: Multitask Learning as Question Answ  A variety of NLP techniques such as GloVe, Sa-Net, Gender Equalizing Loss Function, Choice of Plausible Alternatives, CoQA, Few-Shot Autoregressive Density Estimation, Unanswerable Questions for SQuAD, NumNet, Gender Bias in Coreference Resolution, Optimization as a Model for Few-Shot Learning, Machine Reading Comprehension with Numerical Reasoning, Guide for Conducting Risk Assessments, Improving Language Understanding by Generative Pre-Training, How Much Knowledge Can You Pack Into the Parameters of a Language Model \n",
      "This paper provides a comprehensive list of references related to the latest advancements in natural language processing. It includes papers that discuss new models and methods for language understanding, generation, and reasoning, as well as papers that provide insights into the underlying principles of NLP.  Ziegler et al. (2019) present a method for fine-tuning language models from human preferences, by using reinforcement learning and human feedback. Their approach allows humans to more easily control the behavior of language models, and has potential applications in natural language processing tasks such as dialogue systems and machine translation.\n"
     ]
    }
   ],
   "source": [
    "summary_text = ' '.join(summary_list)\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081fffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We demonstrate that GPT-3, a 175 billion parameter language model, is capable of few-shot learning, achieving strong performance on many NLP tasks without any gradient updates or fine-tuning. We evaluate GPT-3 on over two dozen datasets and show that it can outperform existing models in the zero-shot and one-shot settings, and in some cases can even approach human performance in the few-shot setting. We also identify datasets where GPT-3 struggles and discuss potential risks and limitations of using such large language models.\n"
     ]
    }
   ],
   "source": [
    "# summarize the summary\n",
    "\n",
    "if len(summary_text) > 6000:    # limit prompt size to fit model\n",
    "    summary_text = summary_text[:6000]\n",
    "prompt = summary_text + \"\\n Tl;dr:\"\n",
    "response = openai.Completion.create(\n",
    "    model = \"text-davinci-003\",\n",
    "    prompt = prompt,\n",
    "    temperature = 0.7,\n",
    "    max_tokens = 400,\n",
    "    top_p = 0.9,\n",
    "    frequency_penalty = 0.0,\n",
    "    presence_penalty = 1\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594507b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
